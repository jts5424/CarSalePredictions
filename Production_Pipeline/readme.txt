Lets think of this project in 4 installments
1. Exploratory Data Analysis (data_explore.ipynb files) where detailed analysis of the data structure, types, and validity is completed. This is necessary to execute the appropriate data cleaning, manipulation, and feature engineering proceedures
2. Development Model Comparison (dev_*.py files) where all of the data cleaning, maipulatoin, and feature engineering are first implemented in order to compare the efficacy of various machine learning model. This is where we choose the best performing machine learning algorithm to build a production mode with
3. Production Pipeline Development (prod_*.py) where the machine learning algorithm identified from the Model Development stage is implemented into a pipeline to productionize all procedures including cleaning, manipulation, feature engineering, training, hyperparameter tuning, validation, and testing for final accuracy results review
4. Production Prediction Model Release (predict_sales_price.py) where the production pipeline is integrated into a easily executable file for the end user to make predictions on data for business insights and decision making

HOW TO RUN THE PIPELINE:
1. Specify all variables in prod_input.py appropriately. Some of these input were gathered based on the EDA (data_explore.py). The EDA can be executed on new data to ensure there are no other nuances that need to be accounted for
2. Run prod_model.py
3. Review results of the model validation (cv_results.csv_) and the testing (Random Forest Top Performer Tested Results.csv)
4. for new data input the file path in the filename variable in predict_sales_price.py and run file. Prediction results will be save in a csv file named <filename>_preds

INSIGHTS:
The objective of Challenge Two to predict a continuous variable (sales_price) based on various other variables with different data types and distributions is perfect for a multi-variate regression algorithm. Overall the data turned out to be predictive of sales price. The EDA showed that the variables had significant correlations to sales price in different ways. The categorical variables are nonlinearly correlated because each of their values are not intrinsically related. Some of the numerical ones showed linear or polynomial type trends such as year, power, and engine size. Others show changes in sales_price distribution throughout their domain. This screams non linear regression which was confirmed by fitting various ML models to the data. 
Linear models like lasso, elastic net, and ridge regression did not perform as well as some ensemble models like random forest and gradient boost. Other models like support vector machine landed somewhere in the middle of the pack. This is a perfect use case for the random forest model because of the various ways that these predictors are correlated to sales_price. The random forest can split the data where categories define distinct differences as well as define the trends present in the numerical predictors.
The results show training scores slightly higher than testing scores which indicates that the model is fitting to the training data better than the testing data so there may be some overfitting.
The random forest model tells us that most important predictors are power, engine size and gear type. Logically this makes sense, more expensive cars tend to have strong engines and we did observe a clear difference in the gear type boxplot in the EDA. The year trend is steady and slow so that is probably why it is not as important. As for the name, we do see clear differences from name to name but there is a good bit of scatter and multiple names are at the same level in the box plots so this is probably why it was not as important. As assumed based on the boxplots the location also does not make a big difference.
I decided to feature engineer the 'name' variable s.t. it only contained the car make and not the rest of the title. This feature engineering procedure also helped ease the chances of overfitting the data. If I did not execute this, there would be ~2000 columns in the data with many of those having very few records for a given 'name' value. This is a recipe for overfitting. When comparing the sale_price boxplot grouped by the full name with the one grouped by just the car make we see somewhat of a similar signature (clear differences in sale_price distribution between name values) This indicates that we are probably not losing too much info by truncating the name. Instead of ~1800 unique names, we have ~30.
This model may be able to improve with the use of the new_price column. I decided to remove it all together because so many data points were missing and considering how highly correlated it is to the sale_price I did not think that a simple impute of the mean would do it justice. With more time a model could be made to predict the new_price based on the other predictor variables. 
Another area I would dove further into if I had more time is the validation and reviewing the distribution of the 'Test' set in relation to the predictor variables. This is key because we want to ensure that we are adequately testing the training data domain. Also, seeing where we are getting higher errors would provide insights into where the model is struggling and why. Based on that we could impute more data to make the model more robust.

BUSINESS CASE:
It is hard to say how I would take advantage of the used car sales shop business model without knowing for sure what it is. My best guess, industry professionals use their experience and gut feeling to estimate what a certain car would sell for in a given market and set their maximum buy prices to be lower than that. If this model can do a better job at predicting a sale price than in industry professional then it could be worth something to them. 
To prove this models worth I would try to gain an understanding of how this model stacks up compared the industry professionals best guess and market it to them based on those metrics. You could even create a platform where a used car lot manager reviews a bunch of listings and guesses what the sale prices was, then show them what it actually was. If they realize they are wrong or taking too much time to figure it out on their own then they have reason to purchase access to the productionized prediction platform where they can plug in car spec replicating the input data and recieve a predicted sale price. You could also do this the other way around, by marketing to car buyers so they dont get ripped off.
If the new price data was more full this model could also be used to showcase what cars hold their value better through the first resale. This information would come in handy for car buyers. The model could highlight makes, or certain specs that are more lucrative based on the spread between new price and sale price for both the used car lot and the buyer.